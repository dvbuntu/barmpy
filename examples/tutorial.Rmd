---
title:  '`barmpy` Tutorial'
subtitle: 'Bayesian Additive Regression Networks'
author: Danielle Van Boxel
output: bookdown::pdf_document2
bibliography: lib.bib
toc: false
---

```{r setup, include=FALSE}
# Change all R formatting to 4 decimals
# From https://stackoverflow.com/a/48532968
# Register an inline hook:
knitr::knit_hooks$set(inline = function(x) {
  x <- sprintf("%1.4f", x)
  paste(x, collapse = ", ")
})
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
knitr::knit_engines$set(python = reticulate::eng_python)
```

# Welcome to BARN!

`barmpy` (for Bayesian Additive Regression Models in Python) is the Python module to run various kinds of ensembles trained in a fully Bayesian manner inspired by [BART](https://arxiv.org/abs/0806.3286).  The primary (currently only) method is to use an ensemble of single hidden layer neural networks, hence, BARN, Bayesian Additive Regression Networks.

These are suitable for modeling regression problems in data science.  In particular, BARN excels at both architecture search within the space of neural networks and adapting to a wide variety of problems.

# Background{#sec:back}

Without going into too much mathematical detail (for that, we recommend interested users check out our recent work [@vanboxel2022barn]), we describe some of the important considerations when using `barmpy`.  First, some historical foundations.  BARN is heavily inspired by a method called , "BART", for "Bayesian Additive Regression Trees" [@chipman2010bart].  BART is an ensemble of decision trees, trained with a fully Bayesian procedure.  It fixes the number of trees, and then fixes all but one of them temporarily, subtracting off their contribution from the result.  We then putatively mutate the final tree and evaluate how it performs on the residual.  The size of trees, distribution of their terminal node values, and accuracy of predictions all factor into a posterior probability estimate (along with priors for these components).  We compute this posterior for both the old and mutant tree, taking the ratio as the probability of accepting the new tree (which, if over 1, means we must accept it).  Repeating this process for each other tree in the ensemble completes one iteration of the algorithm.  Iterating this over many iterations ensures convergence to the stationary (i.e. fully Bayesian posterior) distribution, the final step yielding a suitable ensemble.

In BARN, the process is very similar, but we replace the decision trees with single hidden layer neural networks.  This has a few implications.  Our "mutation" operation is a transition between adding or subtracting a neuron from the network.  As such, you need to specify a transition probability for each possibility.  The defaults are 60% for shrinking and 40% for growing, but 50/50 is also reasonable.  Additionally, the prior probability is restricted to the size of the network (currently, we use a flat prior for the weight values and the error term).  By default, we use a Poisson distribution where you specify the expected number of neurons.  For larger networks, set this value higher.  Note, however, that one of the advantages of ensembles is that combining weak learners can be effective at prediction and limiting overfitting.  To that end, consider making the expected number of neurons small (say $N=3$), but with relatively more networks in the ensemble.  These are among the most powerful hyperparameters to tune to improve BARN fits.

# Linear With Irrelevant Features Example{#sec:linear}

To show BARN's capabilities, consider a regression problem that purely linear except the $X$ vector includes irrelevant features.  Techniques like Ordinary Least Squares have no problem with this, but BART itself may falter [@vanboxel2022barn].  First, let's generate some data with a linear form (note no intercept term):

$$
y_i = \sum_{j=1}^{2} \beta_j x_j + 0 x_3 + 0 x_{4} + \epsilon_i
$$

That is, we have 2 explanatory variables with a linear relation to the output, $y_i$.  There is some amount of noise, $\epsilon_i$, and 2 irrelevant features, $x_3$ and $x_{4}$.  We can generate data like this with `sklearn`.

```{python lin_datagen, cache=F, fig.height=4}
import sklearn.datasets as skds
import sklearn.model_selection as skms
import numpy as np
# only for printing purposes should we ignore all warnings
import warnings
warnings.filterwarnings("ignore")

# Setup constants
sig = 1
random_state = 42

# Make the data
X, Y, coef = skds.make_regression(n_samples=1000, n_features=4,
            n_informative=2, noise=sig, random_state=random_state, coef=True)
print(coef) # 2 zeros, 2 nonzero, no intercept
```

Because the relationship here is purely linear, each $\beta_j$ corresponds to the expected change in $y_i$ given a unit change in $x_j$ (i.e. $x_j+1$) while holding the other variables fixed.  For example, if $\beta_1=4$, then increasing $x_1$ by 2 increases the expected value of $y_i$ by 8.

While BARN can automatically divide into training/validation/testing, for clarity, let's do that manually first.  We will use 500 data points for training, 250 for validation, and 250 for final testing.  This is somewhat arbitrary; a more common split with live data might be 80% training, 10% validation, and 10% testing.

```{python lin_split, cache=F, fig.height=4}
# Split into training/validation/testing
Xtr, XX, Ytr, YY = skms.train_test_split(X,Y,
                test_size=0.5,
                random_state=random_state) # training
Xva, Xte, Yva, Yte = skms.train_test_split(XX,YY,
        test_size=0.5,
        random_state=random_state) # valid and test
```

Let's take a look at how each variable corresponds to the data, understanding that this isn't the full picture.

```{python lin_scatter, cache=F, fig.height=5}
import pandas as pd
import matplotlib.pyplot as plt

data = pd.DataFrame(np.concatenate([Ytr.reshape((-1,1)),Xtr],axis=1),
        columns=['Y'] + [f'X_{i+1}' for i in range(Xtr.shape[1])])
axes = pd.plotting.scatter_matrix(data, alpha=0.1)
_ = plt.show()
```

This makes clear there's a strong linear relationship between $Y$ and $X_4$, but we can't immediately see the other relationship.  We can quickly test how ordinary least squares (OLS) performs on this.  It should recover the true parameters reasonably closely and make accurate predictions.

```{python lin_ols, cache=F, fig.height=4}
from sklearn.linear_model import LinearRegression as LR

# create model and fit
ols = LR()
_ = ols.fit(Xtr,Ytr)
print(ols.coef_)
```

Now, to be statistically rigorous, we would compute significance values on these coefficients as well.  This would help us assess if a coefficient estimate is *significantly* different from zero.  It's beyond the scope of this tutorial, but the Python module, `statsmodels` (https://www.statsmodels.org/stable/index.html), handles this for a variety of linear models [@seabold2010statsmodels].

Note that while we fit on the training data, we will evaluate the Mean Squared Error (MSE) on the testing data (validation data is often used for setting hyperparameters, algorithmic settings, which aren't needed for OLS).

```{python lin_mse, cache=F, fig.height=4}
# analyze results
mse_ols_te = np.mean((Yte - ols.predict(Xte))**2)
print(mse_ols_te) # on par with known noise
```

So the OLS parameter estimates are close to the known good values, and error is on the order of the expected noise (note in that practice, one may not known these values *a priori*).  In practice for a problem like this, one would probably stop here!  If OLS meets your needs, don't use a "fancier" method just because it exists.

But let's see how BARN performs.  We call it in a very similar fashion to `sklearn` type models.  First we create the model object, picking a reasonable number of networks as well as transition probabilities.  Let's use 50/50 transition probabilities for this.  Then we train it, and finally, we analyze the results; because the "coefficients" BARN learns are the many weights of an ensemble of neural networks, we focus on just the MSE of BARN on test data as a metric.

```{python lin_barn, cache=F, fig.height=4}
from barmpy.barn import BARN

# create model and fit
barn_model = BARN(num_nets=33,
                trans_probs=[0.5,0.5],
                random_state=42
                )
# l=3 sets expected number of neurons, poisson distributed
# lr=0.1 is a somewhat high learning rate
_ = barn_model.setup_nets(epochs=20, l=3, lr=0.1)
_ = barn_model.train(Xtr,Ytr, Xva, Yva, Xte, Yte,)

# analyze results
pred = barn_model.predict(Xte)
mse_barn_te = np.mean((Yte - pred)**2)
print(mse_barn_te, mse_ols_te) # note BARN note as good in this case
```

In this particular situation, the BARN model has a slightly *higher* test MSE than the OLS model.  This isn't surprising, since the OLS model has the exact right functional form, while BARN is trying to simulate that.  

# Nonlinear function of a single variable{#sec:nonlinear}

As another example, consider a regression problem where the response variable is some nonlinear function a single input variable.  For example:

$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \beta_3 \log (|x_i|+1) + \epsilon_i
$$

If we are aware of this functional relationship, then we could construct additional explanatory variables from the nonlinear components and use OLS to model the response.  But directly supplying $x$ alone would likely fail to produce an accurate model.  Let's see how this works.  First, as before, generate some data:

```{python nonl_datagen, cache=F, fig.height=4}
import sklearn.datasets as skds
import numpy as np
# only for printing purposes should we ignore all warnings
import warnings
warnings.filterwarnings("ignore")

# Setup constants
sig = 1
random_state = 42
rng = np.random.default_rng(random_state)

# Make the data
Xp = rng.normal(size=1000)
eps = rng.normal(scale=sig, size=1000)
```

These calls to `rng.normal` setup original single variable, $X'$, and the noise term, $\epsilon$.  Next we create a nonlinear function of $X'$ as noted above, first stacking the $X'$ components and then taking a dot product with some $\beta$ coefficients.  Finally, we add the noise term to each data point.

```{python nonl_datagen2, cache=F, fig.height=4}
X = np.stack([Xp, Xp**2, np.log(np.abs(Xp)+1)], axis=-1)
beta = np.arange(4)+1
Y = beta[0] + np.dot(X, beta[1:]) + eps
```

From here, we can split the data into training, validation, and testing, as before.  And, because this *is* a function of one variable, we can plot $Y$ vs $X'$ to visualize the output.  Here, it seems the quadratic term, $X'^2$ dominates the shape of the curve.

```{python nonl_datagen3, cache=F, fig.height=4}
# Split into training/validation/testin
Xtr, XX, Ytr, YY = skms.train_test_split(X,Y,
                test_size=0.5,
                random_state=random_state) # training
Xva, Xte, Yva, Yte = skms.train_test_split(XX,YY,
        test_size=0.5,
        random_state=random_state) # valid and test

# plot on a single set of axes
_ = plt.scatter(Xtr[:,0],Ytr, alpha=0.25)
_ = plt.xlabel('X')
_ = plt.ylabel('Y')
_ = plt.show()
```

Now, let's see what OLS does when given all of these features vs only the X input.  To do this, we build two models: the first gets to see all the transformed data, while the latter only has the single original input.


```{python nonl_ols, cache=F, fig.height=4}
# given all variables
ols = LR()
_ = ols.fit(Xtr,Ytr)

# given only original X
olsx = LR()
_ = olsx.fit(Xtr[:,:1],Ytr)

# analyze results
mse_ols_te = np.mean((Yte - ols.predict(Xte))**2)
mse_olsx_te = np.mean((Yte - olsx.predict(Xte[:,:1]))**2)

print(mse_ols_te, mse_olsx_te) # nonlinear model much lower error, on par with noise
```

As expected, only using the single input variable is much worse, because the relationship is nonlinear.  OLS can only model linear relationships.  If you know (or can guess) the nonlinear transformation, it may still be appropriate.  We can further see how these estimated models vary by plotting their response as a function of the original $X$.

```{python nonl_ols_scat, cache=F, fig.height=4}
# Plot on top of data
_ = plt.scatter(Xte[:,0],Yte, alpha=0.25)
_ = plt.xlabel('X')
_ = plt.ylabel('Y')
## make grid of x points in range
Xr1 = np.linspace(np.min(Xp), np.max(Xp), 50)
Xr_all = np.stack([Xr1, Xr1**2, np.log(np.abs(Xr1)+1)], axis=-1)
_ = plt.plot(Xr1, ols.predict(Xr_all), label='Full OLS')
_ = plt.plot(Xr1, olsx.predict(Xr1.reshape([-1,1])), label='1-var OLS')
_ = plt.legend()
_ = plt.show()
```

Now let's try BARN, using similar parameters as we did previously.  We will only allow it to see the variable itself for a fair comparison.  Additionally, we can train a single neural network to see if the ensembling and fully bayesian properties of BARN are helpful.  As in the previous problem, we setup the BARN ensemble object, and then we train it with the provided data.

```{python nonl_barn, cache=F, fig.height=4}
# create model and fit
barn_model = BARN(num_nets=33,
                trans_probs=[0.5,0.5],
                random_state=42
                )
# l=3 sets expected number of neurons, poisson distributed
# lr=0.1 is a somewhat high learning rate
_ = barn_model.setup_nets(epochs=20, l=3, lr=0.1)
_ = barn_model.train(Xtr[:,:1],Ytr, Xva[:,:1], Yva, Xte[:,:1], Yte,)

# straight NN
import sklearn.neural_network as sknn
nn_model = sknn.MLPRegressor([50],
                learning_rate_init=0.1,
                random_state=random_state,
                max_iter=20)
_ = nn_model.fit(Xtr[:,:1],Ytr,)
```

By design, the BARN usage is similar to the `sklearn` usage of neural networks.  But as it has a few extra parameters to tweak, we have an extra step in the setup.  And to emphasize that the fitting process trains neural networks, we call this method `barn_model.train()`.  Now let's review the MSE of these methods.  

```{python nonl_barn2, cache=F, fig.height=4}
# analyze results
pred = barn_model.predict(Xte[:,:1])
mse_barn_te = np.mean((Yte - pred)**2)
print(mse_barn_te) # not as good, but close
pred_nn = nn_model.predict(Xte[:,:1])
mse_nn_te = np.mean((Yte - pred_nn)**2)
print(mse_nn_te) # comparable this time
```

In this particular case, the BARN results are comparable to the full OLS results, despite having only a single variable input.  The large neural network itself is close to this as well, but it comprises many many more weights.  A 50-neuron neural net with 1 input has $1\times 50 + 50 + 50\times 1 + 1=151$ weights.  The total number of weights in BARN varies depending on the final distribution of the neural network sizes in the ensemble.  In this run it was only 66 total weights, yet it outperformed the larger neural network!  This mean that, implemented correctly, BARN can significantly save on inference time (after paying the one-time cost of fully-Bayesian training).

Finally, let's add these models' predictions to the overall Y vs X graph underlaid by data.

```{python nonl_barn3, cache=F, fig.height=4}
# Plot on top of data
_ = plt.scatter(Xte[:,0],Yte, alpha=0.25)
_ = plt.xlabel('X')
_ = plt.ylabel('Y')
## make grid of x points in range
Xr1 = np.linspace(np.min(Xp), np.max(Xp), 50)
Xr_all = np.stack([Xr1, Xr1**2, np.log(np.abs(Xr1)+1)], axis=-1)
_ = plt.plot(Xr1, ols.predict(Xr_all), label='Full OLS')
_ = plt.plot(Xr1, olsx.predict(Xr1.reshape([-1,1])), label='1-var OLS')
_ = plt.plot(Xr1, barn_model.predict(Xr1.reshape([-1,1])), label='BARN')
_ = plt.plot(Xr1, nn_model.predict(Xr1.reshape([-1,1])), label='Big NN')
_ = plt.title("Comparison of Regression Methods on Nonlinear Problem")
_ = plt.legend()
_ = plt.show()
```


\newpage

# References
