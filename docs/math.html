
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Math &#8212; BARMPy 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="BARN" href="barn.html" />
    <link rel="prev" title="Welcome to BARMPy’s documentation!" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="math">
<h1>Math<a class="headerlink" href="#math" title="Permalink to this heading">¶</a></h1>
<p>We briefly describe some of the mathematics behind Bayesian Additive Regression Models like BART and BARN.  For more technical details, please see <a class="reference external" href="https://arxiv.org/abs/0806.3286">Bayesian Additive Regression Trees</a> and <a class="reference external" href="https://arxiv.org/abs/2404.04425">Bayesian Additive Regression Networks</a>.</p>
<p>First, we focus on “regression” problems, where there is floating point output, <span class="math notranslate nohighlight">\(y\)</span>, and a vector of floating point inputs, <span class="math notranslate nohighlight">\(x\)</span>, for each data point.  In general, we posit that there exists some function, <span class="math notranslate nohighlight">\(u\)</span>, such that <span class="math notranslate nohighlight">\(y = u(x) + \epsilon\)</span>, where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a noise term.  Our objective is to find some <em>other</em> function, <span class="math notranslate nohighlight">\(f\)</span>, so that <span class="math notranslate nohighlight">\(f(x) \approx u(x)\)</span>, because if we knew <span class="math notranslate nohighlight">\(u\)</span>, we’d be done already.</p>
<p>An approach like linear regression assumes <span class="math notranslate nohighlight">\(f(x) = \sum w_j x_j\)</span>, i.e. a weighted sum of the inputs.  In ordinary linear regression (OLS), we find the weights, <span class="math notranslate nohighlight">\(w_j\)</span>, that minimize a loss function, <span class="math notranslate nohighlight">\(L = \sum_i (y_i - f(x_i))^2\)</span>, also called the “squared error loss”.  If we average <span class="math notranslate nohighlight">\(L\)</span> over the number of data points, when we have a “mean squared error” or MSE.</p>
<section id="bayesian-additive-regression-models">
<h2>Bayesian Additive Regression Models<a class="headerlink" href="#bayesian-additive-regression-models" title="Permalink to this heading">¶</a></h2>
<p>Our methods like BARN also seek to minimize the MSE, but we posit a different form for <span class="math notranslate nohighlight">\(f\)</span>, so we need a different optimization procedure.  We will in fact learn <span class="math notranslate nohighlight">\(k\)</span> different functions, <span class="math notranslate nohighlight">\(f_1,f_2,\ldots, f_k\)</span> and aggregate them so <span class="math notranslate nohighlight">\(f(x) = \sum_j f_j(x)\)</span>.  This is known as model ensembling.</p>
<p>Well, so what does each <span class="math notranslate nohighlight">\(f_j\)</span> look like, and how do we find them all?  In BART, each <span class="math notranslate nohighlight">\(f_j\)</span> is a “Decision Tree”; it asks a series of questions about the input data in order to arrive at an output result.  In BARN, however, <span class="math notranslate nohighlight">\(f_j\)</span> is a small single hidden layer neural network.  While there exist standard methods to train both of these, we’re going to do something somewhat more involved.</p>
<p>Really, there’s many sets of such models (trees or NNs) that would probably accurately model our data.  But maybe we also have some initial ideas on how big these models should be, because we want to avoid overfitting.  So really, there’s a probability distribution of models that both accurately model the data and account for our prior ideas.  This is exactly a Bayesian “posterior” estimate: the probability of the evidence under the model times the prior probability of the model.</p>
<div class="math notranslate nohighlight">
\[p(f|x,y) \propto p(y|f,x) p(f)\]</div>
<p>The trick is how to sample from this kind of space?  As it’s some probability distribution, we can engineer a Markov Chain that has it as a stationary distribution so that sampling from the chain means we sample from the desired models.  This is known as “Markov Chain Monte Carlo”.  There’s still one issue, however, as in our ensemble, the dimensionality of the space is quite large.  So rather than sample all <span class="math notranslate nohighlight">\(f_1, f_2, \ldots, f_k\)</span> at once, we instead sample one at a time while holding the rest fixed.</p>
<p>Suppose we fix <span class="math notranslate nohighlight">\(f_2, \ldots, f_k\)</span> in whatever their current state is.  Now compute the “residual”, <span class="math notranslate nohighlight">\(r_i = y_i - \sum_{j=2}^k f_j(x_i)\)</span>, which is just whatever is leftover of the target value after subtracting out the predictions of the frozen members of the ensemble.  This gives a new sort of a mini-regression problem with <span class="math notranslate nohighlight">\(x_i,r_i\)</span> and <span class="math notranslate nohighlight">\(f_1\)</span>.</p>
<p><em>Now</em> we can setup an MCMC to find a better <span class="math notranslate nohighlight">\(f_1\)</span> sampling from the posterior.  This requires 3 essential components for both <span class="math notranslate nohighlight">\(M\)</span> (old) and <span class="math notranslate nohighlight">\(M'\)</span> (new):</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(T(M'|M)\)</span>, transition proposal probability (what’s the new model going to look like?)</p></li>
<li><p><span class="math notranslate nohighlight">\(p(y|M',x)\)</span>, error probability (how well does this model fit the data)</p></li>
<li><p><span class="math notranslate nohighlight">\(p(M')\)</span>, prior probability (how likely is this type of model)</p></li>
</ol>
<p>If we have these for both the old and new model, then we can compute an acceptance probability of replacing the old model with the new:</p>
<div class="math notranslate nohighlight">
\[\alpha = min(1, \frac{T(M'|M)p(y|M',x)p(M')}{T(M|M')p(y|M,x)p(M)})\]</div>
<p>If a uniform random number between 0 and 1 is less than this <span class="math notranslate nohighlight">\(\alpha\)</span>, then we set <span class="math notranslate nohighlight">\(f_1=M'\)</span>, else we leave it as <span class="math notranslate nohighlight">\(M'\)</span>.  In this case, we take just a single step; then we will fix <span class="math notranslate nohighlight">\(f_1\)</span>, free <span class="math notranslate nohighlight">\(f_2\)</span>, recompute the residual, and repeat the process.  Cycling through each of the models in the ensemble is one step in the MCMC process; we will probably go for hundreds of iterations.</p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="index.html">
    <img class="logo" src="_static/barmpy_logo.png" alt="Logo"/>
    
  </a>
</p>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Math</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#bayesian-additive-regression-models">Bayesian Additive Regression Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="barn.html">BARN</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="index.html" title="previous chapter">Welcome to BARMPy’s documentation!</a></li>
      <li>Next: <a href="barn.html" title="next chapter">BARN</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, Danielle Van Boxel.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.0.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
    </div>

    

    
  </body>
</html>